{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    @article{ZHENG2023121607,\\n    title = {Interpretable building energy consumption forecasting using spectral clustering algorithm and temporal fusion transformers architecture},\\n    journal = {Applied Energy},\\n    volume = {349},\\n    pages = {121607},\\n    year = {2023},\\n    issn = {0306-2619},\\n    doi = {https://doi.org/10.1016/j.apenergy.2023.121607},\\n    url = {https://www.sciencedirect.com/science/article/pii/S0306261923009716},\\n    author = {Peijun Zheng and Heng Zhou and Jiang Liu and Yosuke Nakanishi},\\n    keywords = {Building energy consumption forecasting, Attention mechanism, Interpretable decomposition method, Interpretable deep learning model},\\n    abstract = {Accurate building energy consumption forecasting is crucial for developing efficient building energy management systems, improving energy efficiency, and local building energy supervision and management. However, short-term building energy consumption forecasting is challenging due to highly non-smooth and volatile trends. In this paper, we present a novel methodology that combines interpretable decomposition methods with an interpretable forecasting model. We first illustrate a daily energy consumption pattern recognition (DECPR) method, which decomposes daily energy consumption patterns into interpretable energy consumption subsequences. To achieve satisfactory forecasting performance, we design the vector representation of each subsequence as a static input to the temporal fusion transformers (TFT) model. This vector representation integrates the DECPR method into the TFT model. The TFT model produces interpretable outputs, such as the attention analysis of different step lengths and the visualization of the importance ranking of exogenous variables, including meteorological data, calendar information, and the vector representation. Empirical studies demonstrate that our proposed DECPR-TFT system outperforms comparable models with a mean absolute percentage error (MAPE) of 6.11%, which is significantly lower than other models. These interpretable outputs provide valuable insights for researchers seeking to develop energy-saving operation strategies in buildings. Overall, our methodology offers a promising solution for short-term building energy consumption forecasting that can contribute to more efficient building energy management and energy-saving operation strategies.}\\n    }\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hyperparameters ref\n",
    "'''\n",
    "    @article{ZHENG2023121607,\n",
    "    title = {Interpretable building energy consumption forecasting using spectral clustering algorithm and temporal fusion transformers architecture},\n",
    "    journal = {Applied Energy},\n",
    "    volume = {349},\n",
    "    pages = {121607},\n",
    "    year = {2023},\n",
    "    issn = {0306-2619},\n",
    "    doi = {https://doi.org/10.1016/j.apenergy.2023.121607},\n",
    "    url = {https://www.sciencedirect.com/science/article/pii/S0306261923009716},\n",
    "    author = {Peijun Zheng and Heng Zhou and Jiang Liu and Yosuke Nakanishi},\n",
    "    keywords = {Building energy consumption forecasting, Attention mechanism, Interpretable decomposition method, Interpretable deep learning model},\n",
    "    abstract = {Accurate building energy consumption forecasting is crucial for developing efficient building energy management systems, improving energy efficiency, and local building energy supervision and management. However, short-term building energy consumption forecasting is challenging due to highly non-smooth and volatile trends. In this paper, we present a novel methodology that combines interpretable decomposition methods with an interpretable forecasting model. We first illustrate a daily energy consumption pattern recognition (DECPR) method, which decomposes daily energy consumption patterns into interpretable energy consumption subsequences. To achieve satisfactory forecasting performance, we design the vector representation of each subsequence as a static input to the temporal fusion transformers (TFT) model. This vector representation integrates the DECPR method into the TFT model. The TFT model produces interpretable outputs, such as the attention analysis of different step lengths and the visualization of the importance ranking of exogenous variables, including meteorological data, calendar information, and the vector representation. Empirical studies demonstrate that our proposed DECPR-TFT system outperforms comparable models with a mean absolute percentage error (MAPE) of 6.11%, which is significantly lower than other models. These interpretable outputs provide valuable insights for researchers seeking to develop energy-saving operation strategies in buildings. Overall, our methodology offers a promising solution for short-term building energy consumption forecasting that can contribute to more efficient building energy management and energy-saving operation strategies.}\n",
    "    }\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from darts.dataprocessing.transformers import Scaler\n",
    "from darts.models import RNNModel\n",
    "from darts.models import NBEATSModel\n",
    "from darts.datasets import WeatherDataset\n",
    "from darts.models import LinearRegressionModel\n",
    "import darts.metrics as metrics\n",
    "from darts.datasets import AirPassengersDataset\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import optuna\n",
    "import torch\n",
    "from optuna.integration import PyTorchLightningPruningCallback\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "\n",
    "from darts.dataprocessing.transformers import Scaler\n",
    "from darts.datasets import AirPassengersDataset\n",
    "from darts.models import TCNModel\n",
    "from darts.utils.likelihood_models import GaussianLikelihood\n",
    "\n",
    "from darts.models import TFTModel\n",
    "from darts.utils.likelihood_models import QuantileRegression\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "from darts import TimeSeries\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_base():\n",
    "    # set basic settings:\n",
    "    # 1. model identification\n",
    "    # 2. params of model, and whether call optuna\n",
    "    # 3. prediction settings\n",
    "    # 2. saving settings\n",
    "    def __init__(self,id, model_type,\n",
    "                 call_optuna, optuna_settings, optuna_params_dic,\n",
    "                 call_one_trial, one_trial_settings, one_trial_params_dic,\n",
    "                 load_from_trained_model,load_settings,\n",
    "                 load_from_checkpoint,load_checkpoint_settings,\n",
    "                 predict_settings,metric_settings, save_settings):\n",
    "        # for optuna params_dic should be like:\n",
    "        #   dic={\n",
    "        #       'param_key':[tpye, lower_bound, upper_bound, bool_log], #params to search\n",
    "        #       'param_key':[value] #params not to search\n",
    "        #   }\n",
    "        # check if necessary key pairs are passed\n",
    "        \n",
    "        for i in ['folder','save_model','save_prediction','save_metrics']:\n",
    "            assert i in save_settings.keys()\n",
    "            \n",
    "        self.model_type=model_type\n",
    "        self.model=None # tmp model for optuna / model load from file\n",
    "        \n",
    "        self.save_settings=save_settings\n",
    "        self.save_path=os.path.join(save_settings['folder'],id)\n",
    "        self.id=id\n",
    "        if not os.path.exists(self.save_path):\n",
    "            os.makedirs(self.save_path)\n",
    "        \n",
    "        self.call_optuna=call_optuna\n",
    "        if call_optuna==True:\n",
    "            assert optuna_settings!=None\n",
    "            assert optuna_params_dic!=None\n",
    "        self.optuna_params_dic=optuna_params_dic\n",
    "        self.optuna_settings=optuna_settings\n",
    "        self.optuna_model_name=None\n",
    "        \n",
    "        self.call_one_trial=call_one_trial\n",
    "        if call_one_trial==True:\n",
    "            assert one_trial_settings!=None\n",
    "            assert one_trial_params_dic!=None\n",
    "        self.one_trial_params_dic=one_trial_params_dic\n",
    "        self.one_trial_settings=one_trial_settings\n",
    "        self.one_trial_model_name=None\n",
    "        \n",
    "        self.load_from_trained_model=load_from_trained_model\n",
    "        if load_from_trained_model==True:\n",
    "            assert load_settings!=None\n",
    "            self.load_settings=load_settings\n",
    "            self.load_from_model()\n",
    "            \n",
    "        self.load_from_checkpoint=load_from_checkpoint\n",
    "        if load_from_checkpoint==True:\n",
    "            assert load_checkpoint_settings!=None\n",
    "            self.load_checkpoint_settings=load_checkpoint_settings\n",
    "            self.load_from_checkpoints()\n",
    "        \n",
    "        \n",
    "        self.predict_settings=predict_settings\n",
    "        self.metric_settings=metric_settings\n",
    "        \n",
    "\n",
    "        \n",
    "        self.optuna_study=None\n",
    "        self.best_params=None\n",
    "        self.best_model=None\n",
    "        \n",
    "        self.one_trial_model=None # model from run_one_trial\n",
    "        \n",
    "        ...\n",
    "    def load_from_model(self):\n",
    "        self.model=self.model_type.load(**self.load_settings)\n",
    "        ...\n",
    "        \n",
    "    def load_from_checkpoint(self):\n",
    "        self.model=self.model_type.load_from_checkpoint(**self.load_checkpoint_settings)\n",
    "    # run one trial on designated params\n",
    "    def run_one_trial(self):\n",
    "        now=datetime.datetime.now()\n",
    "        self.one_trial_model_name='-'.join([str(now.month),str(now.day),str(now.hour),str(now.minute),'one_trial'])\n",
    "        \n",
    "        self.one_trial_model=self.model_type(work_dir=self.save_path,\n",
    "                                             model_name=self.one_trial_model_name, log_tensorboard=True,\n",
    "                                             save_checkpoints=True,\n",
    "                                             **self.one_trial_params_dic)\n",
    "        #self.one_trial_model, optimizer = amp.initialize(self.one_trial_model, optimizer, opt_level='O1')\n",
    "\n",
    "        self.one_trial_model.fit(**self.one_trial_settings)\n",
    "        \n",
    "    # run hyperparams searching\n",
    "    def run_optuna(self):\n",
    "        p=self.optuna_params_dic\n",
    "        \n",
    "        X_train=self.optuna_setting['X_train']\n",
    "        y_train=self.optuna_setting['y_train']\n",
    "        metric_cv=self.optuna_setting['metric_cv']\n",
    "        n_trials=self.optuna_setting['n_trials']\n",
    "        stop_threshold=self.optuna_setting['stop_threshold']\n",
    "        direction=self.optuna_setting['direction']\n",
    "        \n",
    "        def objective(trial,p):\n",
    "            params={}\n",
    "            for i in p:\n",
    "                \n",
    "                if len(p[i])==1:\n",
    "                    params.update({i,p[i][0]})\n",
    "                elif len(p[i])==4:\n",
    "                    assert isinstance(p[i][1],p[i][0])\n",
    "                    assert isinstance(p[i][2],p[i][0])\n",
    "                    if p[i][0]==int:\n",
    "                        params.update({i,trial.suggest_int(i,p[i][1],p[i][2],p[i][3])})\n",
    "                    elif p[i][0]==float:\n",
    "                        params.update({i,trial.suggest_float(i,p[i][1],p[i][2],p[i][3])})\n",
    "                    elif p[i][0]==bool:\n",
    "                        params.update({i,trial.suggest_float(i,p[i][1],[p[i][2],p[i][3]])})\n",
    "                    else:\n",
    "                        Warning(\"Invalid param type!\")\n",
    "            self.model=self.model_type(**params)\n",
    "            self.model.fit(**self.one_trial_settings)\n",
    "            \n",
    "            scores=cross_val_score(self.model, X_train, y_train,\n",
    "                                   cv=KFold(n_split=5,shuffle=True),\n",
    "                                   scoring=metric_cv)\n",
    "            \n",
    "            report_cross_validation_scores(trial, scores)\n",
    "            return scores.mean()\n",
    "        \n",
    "        def callback(study, trial):\n",
    "            for ii, t in enumerate(study.trials):\n",
    "                if t.value >= stop_threshold:\n",
    "                    study.stop()\n",
    "                    \n",
    "        study = optuna.create_study(directions=direction)\n",
    "        \n",
    "        study.optimize(objective, n_trials=n_trials, gc_after_trial=True,\n",
    "                   callbacks=[callback])\n",
    "    \n",
    "        print('Number of finished trials: {}'.format(len(study.trials)))\n",
    "        print('Best trial:')\n",
    "        trial = study.best_trial\n",
    "\n",
    "        print('  Value: {}'.format(trial.value))\n",
    "        print('  Params: ')\n",
    "\n",
    "        for key, value in trial.params.items():\n",
    "            print('    {}: {}'.format(key, value))\n",
    "            \n",
    "        self.optuna_study=study\n",
    "        return study\n",
    "    \n",
    "    def refit_best_trial(self):\n",
    "        assert self.optuna_study!=None\n",
    "        best_trail=self.optuna_study.best_trial\n",
    "        best_params=best_trail.params\n",
    "        #best_params['tree_method']='gpu_hist'\n",
    "        self.best_params=best_params\n",
    "        model=self.model_type(**self.best_params)\n",
    "        model.fit(self.optuna_setting['X_train'],self.optuna_setting['y_train'])\n",
    "        self.best_model=model\n",
    "        return model\n",
    "    \n",
    "    def save_one_trial_model(self):\n",
    "        assert self.one_trial_model !=None\n",
    "        model_path=os.path.join(self.save_path,self.one_trial_model_name,'trained_model')\n",
    "        if not os.path.exists(model_path):\n",
    "            os.makedirs(model_path)\n",
    "        model_name=self.one_trial_model_name\n",
    "        model_name=os.path.join(model_path,model_name+'.pt')\n",
    "        self.one_trial_model.save(model_name)\n",
    "        \n",
    "    def predict_on_one_trial_model(self):\n",
    "        self.prediction=self.one_trial_model.predict(**self.predict_settings)\n",
    "        self.prediction.to_csv(os.path.join(self.save_path,self.one_trial_model_name+'prediction.csv'))\n",
    "        \n",
    "    def predict_on_best_model(self):\n",
    "        self.prediction=self.best_model.predict(**self.predict_settings)\n",
    "        self.prediction.to_csv(os.path.join(self.save_path,self.optuna_model_name+'prediction.csv'))\n",
    "        \n",
    "    def predict_on_loaded_model(self):\n",
    "        self.prediction=self.model.predict(**self.predict_settings)\n",
    "        self.prediction.to_csv(os.path.join(self.save_path,'loaded_model_prediction.csv'))\n",
    "\n",
    "    def cal_metrics(self):\n",
    "        assert self.prediction!=None\n",
    "        metrics_method_dic={\n",
    "            'CV':metrics.coefficient_of_variation,\n",
    "            'MAE':metrics.mae,\n",
    "            'MAPE':metrics.mape,\n",
    "            'OPE':metrics.ope,\n",
    "            'RMSE':metrics.rmse,\n",
    "            'MSE':metrics.mse,\n",
    "            'MARRE':metrics.marre,\n",
    "            'MASE':metrics.mase,\n",
    "            'R2':metrics.r2_score,\n",
    "            'SMAPE':metrics.smape,\n",
    "        }\n",
    "        metrics_dic={\n",
    "            'start_time':self.prediction.time_index[0],\n",
    "            'end_time':self.prediction.time_index[-1],\n",
    "            'n':len(self.prediction.time_index),\n",
    "        }\n",
    "        \n",
    "        for metric in metrics_method_dic.keys():\n",
    "            try:\n",
    "                if metric=='MASE':\n",
    "                    value=metrics_method_dic[metric](self.metric_settings['series_pred_gt'],self.prediction,intersect=True,\n",
    "                                                              insample=self.metric_settings['series_train'], m=96*7)\n",
    "                    print({metric: value})\n",
    "                    metrics_dic.update({metric: value})\n",
    "                else:\n",
    "                    value=metrics_method_dic[metric](self.metric_settings['series_pred_gt'],self.prediction,intersect=True)\n",
    "                    print({metric: value})\n",
    "                    metrics_dic.update({metric: value})\n",
    "            except:\n",
    "                print(\"Fail to calculate metric: {} of model {}\".format(metric,self.id))\n",
    "                \n",
    "        metrics_df=pd.DataFrame([metrics_dic]).T\n",
    "        metrics_df.to_csv(os.path.join(self.save_path,self.id+'_metrics.csv'))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data preperation\n",
    "bld_pd=pd.read_csv(r'/root/autodl-tmp/data/load_prediction_base/BLD_Sum.csv')\n",
    "bld_pd.sort_values(by='DateTime')\n",
    "bld_pd=bld_pd.drop(columns=['RealPower_before_scaling'])\n",
    "bld=TimeSeries.from_dataframe(bld_pd,time_col=\"DateTime\",freq=\"15min\",fill_missing_dates=True)\n",
    "\n",
    "bld=bld.drop_columns(['wind_speed', 'wind_deg', 'rain_1h',\n",
    "       'rain_3h', 'snow_1h', 'snow_3h', 'clouds_all', 'weather_main',\n",
    "       'RealPower_-0d_0h'])\n",
    "\n",
    "transformer = Scaler()\n",
    "\n",
    "# data split\n",
    "train_start=pd.Timestamp(2017,1,1,0,0)\n",
    "train_end=pd.Timestamp(2018,12,31,23,45)\n",
    "\n",
    "val_start=pd.Timestamp(2018,9,1,0,0)\n",
    "val_end=pd.Timestamp(2018,12,31,23,45)\n",
    "\n",
    "pred_start=pd.Timestamp(2019,1,1,0,0)\n",
    "pred_end=pd.Timestamp(2019,12,31,23,45)\n",
    "\n",
    "bld_train=bld[train_start:train_end]\n",
    "bld_val=bld[val_start:val_end]\n",
    "bld_pred=bld[pred_start:pred_end]\n",
    "\n",
    "bld_trian=transformer.fit_transform(bld_train)\n",
    "bld_val=transformer.fit_transform(bld_val)\n",
    "#bld_trian=transformer.fit_transform(bld_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['RealPower', 'is_holiday', 'hour_cos', 'hour_sin', 'dayofweek_cos',\n",
       "       'dayofweek_sin', 'quarter_cos', 'quarter_sin', 'month_cos', 'month_sin',\n",
       "       'dayofmonth_cos', 'dayofmonth_sin', 'temp', 'feels_like', 'temp_min',\n",
       "       'temp_max', 'pressure', 'humidity'],\n",
       "      dtype='object', name='component')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bld.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | criterion     | MSELoss          | 0     \n",
      "1 | train_metrics | MetricCollection | 0     \n",
      "2 | val_metrics   | MetricCollection | 0     \n",
      "3 | stacks        | ModuleList       | 159 M \n",
      "---------------------------------------------------\n",
      "7.0 M     Trainable params\n",
      "152 M     Non-trainable params\n",
      "159 M     Total params\n",
      "637.876   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd9fceeb56b1496582c9f69e5adc0e8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=4` reached.\n"
     ]
    }
   ],
   "source": [
    "model_type=NBEATSModel\n",
    "call_optuna=False\n",
    "optuna_settings=None\n",
    "optuna_params_dic=None\n",
    "call_one_trial=False\n",
    "quantiles = [\n",
    "    0.01,0.05,0.1,0.15,0.2,0.25,0.3,0.4,0.5,\n",
    "    0.6,0.7,0.75,0.8,0.85,0.9,0.95,0.99,\n",
    "]\n",
    "\n",
    "one_trial_settings={\n",
    "    'series':bld_train['RealPower'],\n",
    "    'past_covariates':bld_train.drop_columns(['RealPower']),\n",
    "    #'val_series':bld_val['RealPower'],\n",
    "    #'val_past_covariates':bld_val.drop_columns(['RealPower']),\n",
    "    \n",
    "}\n",
    "my_stopper = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=5,\n",
    "    min_delta=1e-1,\n",
    "    mode='min',\n",
    ")\n",
    "\n",
    "one_trial_params_dic={\n",
    "    'optimizer_kwargs':{\n",
    "        'lr':1e-4,\n",
    "    },\n",
    "    'pl_trainer_kwargs':{\n",
    "        \"callbacks\": [my_stopper],\n",
    "        \"gradient_clip_val\":0.1},\n",
    "    'batch_size':16,\n",
    "    'dropout':0.2,\n",
    "    'input_chunk_length':96*7,\n",
    "    'output_chunk_length':96,\n",
    "    'num_blocks':1,\n",
    "    'num_blocks':1,\n",
    "    'generic_architecture':False,\n",
    "    #'loss_fn':torch.nn.mape(),\n",
    "    'n_epochs':4,\n",
    "    \n",
    "    'pl_trainer_kwargs':{\n",
    "      \"accelerator\": \"gpu\",\n",
    "      \"devices\": [0]\n",
    "    },\n",
    "    'random_state':42,\n",
    "}\n",
    "predict_settings={\n",
    "    'n':96*365,\n",
    "    'series':bld_train['RealPower'],\n",
    "    'past_covariates':bld.drop_columns(['RealPower']),\n",
    "    'n_jobs':-1,\n",
    "    'num_samples':1\n",
    "}\n",
    "metric_settings={\n",
    "    'series_pred_gt':bld_pred['RealPower'],\n",
    "    'series_train':bld_train['RealPower'],\n",
    "}\n",
    "save_settings={\n",
    "    'folder':r'/root/autodl-tmp/load_forecast/N-Beats',\n",
    "    'save_model':True,'save_prediction':True,'save_metrics':True\n",
    "}\n",
    "test=Model_base(\n",
    "    'test_ep_4',\n",
    "    model_type,\n",
    "    call_optuna,\n",
    "    optuna_settings,\n",
    "    optuna_params_dic,\n",
    "    call_one_trial,\n",
    "    one_trial_settings,\n",
    "    one_trial_params_dic,\n",
    "    False,\n",
    "    None,\n",
    "    False,\n",
    "    None,\n",
    "    predict_settings,\n",
    "    metric_settings,\n",
    "    save_settings\n",
    ")\n",
    "test.run_one_trial()\n",
    "test.save_one_trial_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_settings={\n",
    "    'n':96*365,\n",
    "    'series':bld[train_start:train_end]['RealPower'],\n",
    "    'past_covariates':bld.drop_columns(['RealPower']),\n",
    "    'n_jobs':-1,\n",
    "    'num_samples':1\n",
    "}\n",
    "metric_settings={\n",
    "    'series_pred_gt':bld[pred_start:pred_end]['RealPower'],\n",
    "    'series_train':bld[train_start:train_end]['RealPower'],\n",
    "}\n",
    "test.predict_settings=predict_settings\n",
    "test.metric_settings=metric_settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91ca2fb27d574a669e10436c998fe535",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test.predict_on_one_trial_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'CV': 36.46264186542229}\n",
      "{'MAE': 23.30571111490727}\n",
      "{'MAPE': 29.946977573850337}\n",
      "{'OPE': 31.775996697458492}\n",
      "{'RMSE': 26.517354276362493}\n",
      "{'MSE': 703.1700778181203}\n",
      "{'MARRE': 32.46232731962719}\n",
      "{'MASE': 5.004396700819746}\n",
      "{'R2': -1.7408876572286927}\n",
      "{'SMAPE': 36.21083881170582}\n"
     ]
    }
   ],
   "source": [
    "test.cal_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'CV': 54.22939931808551}\n",
      "{'MAE': 36.76198320180292}\n",
      "{'MAPE': 48.746828751915054}\n",
      "{'OPE': 50.541058092633115}\n",
      "{'RMSE': 39.43817892349939}\n",
      "{'MSE': 1555.3699568019515}\n",
      "{'MARRE': 51.20545456569353}\n",
      "{'MASE': 7.89383969206663}\n",
      "{'R2': -5.062678790671568}\n",
      "{'SMAPE': 65.46570250518819}\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'CV': 29.361355272711968}\n",
      "{'MAE': 16.797145911976166}\n",
      "{'MAPE': 20.594195190865115}\n",
      "{'OPE': 20.46374086949731}\n",
      "{'RMSE': 21.352963470784154}\n",
      "{'MSE': 455.94904898464245}\n",
      "{'MARRE': 23.39660205782472}\n",
      "{'MASE': 3.606823287675333}\n",
      "{'R2': -0.7772444535252421}\n",
      "{'SMAPE': 23.844303497788964}\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
